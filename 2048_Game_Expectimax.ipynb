{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - 2048 Solver with Expectimax Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexandra Pukhova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules of 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2048 is a single-player game, (normally) played on a 4x4 board. At initiation, two out of 16 tiles are occupied with blocks. These blocks have values of either 2 or 4 (i.e. two 2s, two 4s, or one of each kind). The player has to slide all exisiting blocks across the board in one direction. Once they pick a direction, all the tiles move that way until they reach an obstacle, i.e. hit another tile or the edge of the board. If there are two tiles of the same value on the way, they will merge and become a tile whose value is the combined value of the two colliding tiles. After the player's move, a new block will randomly appear in an empty spot with a value of either 2 or 4. Once the player reaches the value of 2048 in one of the blocks, they win and can either stop or continue to play in an attempt to reeach a higher value. In this implementation of the game, 2048 is the terminal state. Thus, the game ends when there are no possible slides left or when the terminal state is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the adversary is not optimal in this case - the computer randomly generates the tiles on the board after every move a player makes, I used expectimax search to model the unpredictable behavior of the \"opponent\". In comparison with minimax, the min nodes, in this case, take \"random steps\". In expectimax, we perform optimization over all possible moves, based on their utility. A maximization step is followed by an expectation step that uses the probability of new tiles appearing (in this case, 90% for 2s and 10% for 4s).\n",
    "\n",
    "\n",
    "One shortcoming of the expectimax algorithm is the unavailability of alpha-beta pruning, given that the adversary is not playing optimally. As a result, expectimax suffers from suboptimal speed, so we have to use depth-limited search in order to compensate for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following heuristics are used in this implementation of expectimax to compute the score for each configuration. A combination of those scores is then used to determine the optimal steps for the max nodes to take.\n",
    "\n",
    "1. **Monotonicity**: This heuristic rewards ordering of tiles in descending order in rows and columns. It also gives extra points to configurations, where the maximum value tile is located in the leftmost corner which is aligned with the descending order goal.\n",
    "2. **Smoothness**: This heuristic rewards small value differences in neighbouring cells.\n",
    "\n",
    "The two heuristics are most effective when applied together: whereas the monotonicity heurisitc ensures that the tiles are well-positioned for merging, i.e. organized in a descending value order, the smoothness heuristic ensures that the values are as close as possible to each other, so that merging can actually happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the necessary imports\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from numpy.random import choice\n",
    "import copy\n",
    "import heapq\n",
    "import itertools\n",
    "from itertools import accumulate\n",
    "import time\n",
    "import operator\n",
    "from operator import mul\n",
    "import math\n",
    "from math import log\n",
    "from cachetools import cached\n",
    "from IPython.display import clear_output\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board:\n",
    "    \n",
    "    '''\n",
    "    This is a node class. It captures the state of\n",
    "    the 2048 grid and enables the user to print\n",
    "    the grid state. It also has the methods that\n",
    "    allow us to check whether two board configurations\n",
    "    are equal, to check whether there are any moves\n",
    "    possible left on the board, and to insert new tiles.\n",
    "    \n",
    "    Inputs\n",
    "    \n",
    "        n (int) Single grid dimention.\n",
    "            Default: 4.\n",
    "        \n",
    "        state (arr) Current state of the board of dimentions n-by-n.\n",
    "            Default: None.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n=4, state=None):\n",
    "        self.n = n \n",
    "        \n",
    "        # Originating the board state if none given\n",
    "        if state is None:\n",
    "            self.state = np.zeros((self.n,self.n),dtype = int)\n",
    "            while np.count_nonzero(self.state) < 2:\n",
    "                self.insert_tile()\n",
    "        else:\n",
    "            self.state = state\n",
    "    \n",
    "    def __str__(self):     \n",
    "        '''A printing method.'''\n",
    "        return print(*self.state, sep='\\n')\n",
    "    \n",
    "    def equal(self, other):\n",
    "        '''A method for checking whether the two board states are the same.'''\n",
    "        return(np.array_equal(self.state, other.state))\n",
    "    \n",
    "    def can_move(self):\n",
    "        \n",
    "        n = len(self.state)\n",
    "        truth_vals = []\n",
    "        \n",
    "        # Checking for a zero tile\n",
    "        for row in range(n):\n",
    "            for i in range(n):\n",
    "                if self.state[row,i]==0:\n",
    "                    return True\n",
    "                else:\n",
    "                    # If no zero tiles, check whether there are neighboring tiles of same value\n",
    "                    indices = [[0,-1],[0,1],[-1,0],[1,0]]\n",
    "                    neighbour_vals = []\n",
    "\n",
    "                    for sublist in indices:\n",
    "                        new_pos = [row+sublist[0],i+sublist[1]]\n",
    "                        if ((0<=new_pos[0] and new_pos[0]<=(n-1)) and (0<=new_pos[1] and new_pos[1]<=(n-1))):\n",
    "                            neighbour_vals.append(self.state[row+sublist[0],i+sublist[1]])\n",
    "                    if any(x==self.state[row,i] or x==0 for x in neighbour_vals):\n",
    "                        truth_vals.append(True) \n",
    "                    else: truth_vals.append(False)\n",
    "        return any(i for i in truth_vals)\n",
    "    \n",
    "    def is_terminal_state(self):\n",
    "        return(2048 in self.state)\n",
    "    \n",
    "    def insert_tile(self, added=False):\n",
    "        '''A method that randomly inserts a new tile after each move.'''\n",
    "        \n",
    "        if added: return\n",
    "        \n",
    "        # if no empty tiles left\n",
    "        if (self.n*self.n-np.count_nonzero(self.state))==0: return\n",
    "    \n",
    "        coords = [int(np.random.choice(self.n, 1)), int(np.random.choice(self.n, 1))]\n",
    "        if self.state[coords[0],coords[1]]==0:\n",
    "            self.state[coords[0],coords[1]]=choice([2, 4],p=[0.9,0.1])\n",
    "            added = True\n",
    "        self.insert_tile(added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step & Merge functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(lst, direction):\n",
    "    n=len(lst)\n",
    "    lst = list(filter(lambda a: a != 0, lst))\n",
    "    merge_flag = False\n",
    "\n",
    "    if len(lst)==0: # if no tiles in this row/column\n",
    "        return list(np.zeros(n, dtype='int'))\n",
    "    \n",
    "    if direction >0: # if moving down/right, reverse lst\n",
    "        lst = lst[::-1] \n",
    "    i=0\n",
    "    while i<len(lst)-1:\n",
    "        if lst[i]==lst[i+1]:\n",
    "            lst[i]*=2 # new tile w/ 2x value\n",
    "            del lst[i+1] # delete the old value\n",
    "            merge_flag = True\n",
    "        i+=1\n",
    "    arr_zeros = list(np.zeros(n-len(lst), dtype='int'))\n",
    "    \n",
    "    if direction >0:\n",
    "        if merge_flag:\n",
    "            lst = lst\n",
    "        lst = lst + arr_zeros\n",
    "        lst = lst[::-1]\n",
    "    else:\n",
    "        lst = lst + arr_zeros\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(current_state, direct):\n",
    "    n = len(current_state)\n",
    "    direction = np.asarray(direct)\n",
    "    move_axis = int(np.where(direction!= 0)[0])\n",
    "    next_state = np.zeros((n,n),dtype = int)\n",
    "\n",
    "    if not move_axis: # if 0, move up/down\n",
    "        cols = range(0,n) \n",
    "        for col in cols:\n",
    "            lst = current_state[:, col]\n",
    "            next_state[:, col] = merge(lst, direction[move_axis])\n",
    "    \n",
    "    else: # move left/right\n",
    "        rows = range(0,n)\n",
    "        for row in rows:\n",
    "            lst = current_state[row, :]\n",
    "            next_state[row, :] = merge(lst, direction[move_axis])\n",
    "\n",
    "    return(next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_LIMIT = 2 \n",
    "# The total depth = 2 x DEPTH_LIMIT, since both max and exp steps will be executed twice.\n",
    "# in this case, the depth is 4. \n",
    "\n",
    "def max_value(state, curr_depth=0):\n",
    "    '''The maximization step.'''\n",
    "    val = -np.inf\n",
    "    \n",
    "    potential_moves = {'up':[-1,0],'down':[1,0],'right':[0,1],'left':[0,-1]}\n",
    "    next_states_temp = [step(state, potential_moves[direct]) for direct in potential_moves] # all next states\n",
    "    \n",
    "    # Exclude impossible next_states\n",
    "    next_states = [k for k in next_states_temp if not np.array_equal(state,k)]\n",
    "    \n",
    "    curr_depth +=1\n",
    "    bestmove = state\n",
    "    \n",
    "    if len(next_states) == 0:\n",
    "        return (0, bestmove) # If we reach a leaf node (impossible state), return val = 0 instead of -inf\n",
    "        \n",
    "    for next_state in next_states:\n",
    "        candidatevalue, pos_state = exp_value(next_state,curr_depth)\n",
    "        if candidatevalue > val: \n",
    "            val = candidatevalue\n",
    "            bestmove = pos_state\n",
    "    return (val, bestmove)\n",
    "\n",
    "def exp_value(state, curr_depth):\n",
    "    '''The expectation step.'''\n",
    "    \n",
    "    if curr_depth == DEPTH_LIMIT or (2048 in state):\n",
    "        return (h_sum(state), state)\n",
    "    \n",
    "    val = 0\n",
    "    zero_positions = np.argwhere(state == 0) # get zero positions\n",
    "\n",
    "    for pos in zero_positions:\n",
    "        state_copy = copy.deepcopy(state)\n",
    "        state_copy[pos[0],pos[1]]=2\n",
    "        val += max_value(state_copy,curr_depth)[0]*0.9\n",
    "        state_copy[pos[0],pos[1]]=4\n",
    "        val += max_value(state_copy,curr_depth)[0]*0.1\n",
    "    return (val, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game():\n",
    "    '''\n",
    "    This is a game class that initiates the\n",
    "    board of class Board() and uses expectimax \n",
    "    algorithm until the board reaches the winning state\n",
    "    or an unmovable board configuration.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.board = Board()\n",
    "        \n",
    "    def play(self):\n",
    "        start = time.time()\n",
    "        \n",
    "        while not self.board.is_terminal_state():\n",
    "            \n",
    "            clear_output()            \n",
    "            print(self.board.state, flush=True)\n",
    "            \n",
    "            if not self.board.can_move():\n",
    "                finish = time.time()\n",
    "                print('Time taken: ', (finish-start))\n",
    "                return('Game over :(')\n",
    "                break\n",
    "            \n",
    "            # expectimax step - returns the next state after the optimal move\n",
    "            move = max_value(state=self.board.state)[1]\n",
    "            \n",
    "            # update the board state\n",
    "            self.board.state = move\n",
    "    \n",
    "            # randomly insert new tile\n",
    "            self.board.insert_tile()\n",
    "            \n",
    "        finish = time.time()\n",
    "        \n",
    "        print('Time taken to achieve the goal: ', (finish-start))\n",
    "        print(self.board.state)\n",
    "        return('You won!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using @cached decorator to memoize the outputs of the heuristics\n",
    "# and optimize the run time\n",
    "\n",
    "@cached(cache={}, key=lambda state: tuple(state.flatten()))\n",
    "def h_monoton(state):\n",
    "    '''Ensures that the tile values are in an ascending/descending order.'''\n",
    "    lin_board = (state).flatten()\n",
    "    logs = [log(i,2) if i !=0 else 0 for i in lin_board]\n",
    "    gp = list(accumulate([2]*(len(lin_board)), mul))[::-1] # geometric sequence w/ common ratio 2\n",
    "    \n",
    "    # Compute the dot product of board values and gp.\n",
    "    # I took logs with base two for the board values,\n",
    "    # given the high magnitude of pure values.\n",
    "    # Took the ordination idea from (Pezzotti, 2014).\n",
    "    m_val = np.dot([logs], gp)/np.count_nonzero(state)\n",
    "    \n",
    "    max_corner = 0\n",
    "    if max(lin_board) == lin_board[0]: \n",
    "        max_corner = (log(lin_board[0],2) * gp[0])\n",
    "    \n",
    "    corner_2 = 0\n",
    "    if max_corner != 0:\n",
    "        corner_2 = (log(lin_board[0],2) * gp[0])\n",
    "    \n",
    "    if max_corner == 0:\n",
    "        max_corner += 1\n",
    "    return m_val, max_corner, corner_2\n",
    "    \n",
    "@cached(cache={}, key=lambda state: tuple(state.flatten()))\n",
    "def h_smoothness(state):\n",
    "    '''Minimizes the difference between adjacent cells.'''\n",
    "    \n",
    "    sum_per_row = 0\n",
    "    sum_per_col = 0\n",
    "\n",
    "    for idx in range(len(state)):\n",
    "        row = [log(i,2) if i!=0 else 0 for i in state[idx]]\n",
    "        sum_per_row += sum(np.abs(np.diff(row)))\n",
    "\n",
    "        col = [log(i,2) if i!=0 else 0 for i in state[:,idx]]\n",
    "        sum_per_col += sum(np.abs(np.diff(col)))\n",
    "\n",
    "    total_sum = sum_per_row + sum_per_col\n",
    "    return total_sum\n",
    "\n",
    "def h_sum(state):\n",
    "    '''Sums all the heuristic values per configuration.'''\n",
    "    smooth_val = h_smoothness(state)\n",
    "    smoothness = 1/smooth_val if smooth_val else 0\n",
    "    mon = h_monoton(state)\n",
    "    \n",
    "    # The weights for the sum were chosen arbitrarily,\n",
    "    # based on several runs and their outcomes. In theory,\n",
    "    # we could run Monte Carlo simulations to find the optimal values.\n",
    "    # However, it would be very computationally expensive.\n",
    "    return mon[0]*0.4 + log(mon[1]) + mon[2] + smoothness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1024    0    0    0]\n",
      " [1024   32    2    0]\n",
      " [  64    8    2    4]\n",
      " [   8    2   64    2]]\n",
      "Time taken to achieve the goal:  29.449549198150635\n",
      "[[2048   32    4    4]\n",
      " [  64    8   64    2]\n",
      " [   8    2    0    0]\n",
      " [   0    0    0    2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You won!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints out the last two steps if winning configuration\n",
    "\n",
    "g = Game()\n",
    "g.play()\n",
    "\n",
    "# cProfile.run(\"g.play()\", sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Speed improvement**. The time complexity of the algorithm increases exponentially, making even the depth 6 algorithm considerably slower than the depth 4 one. Thus, implementing some time speeding techniques, using dynamic programming in as many steps of the game as possible would be preferable. The time complexity for DP will be given by the number of unique states multiplied by the amount of time, taken to compute each unique state.\n",
    "\n",
    "    * When I ran depth 6, the algorithm reached the winning configuration much more frequently, however, given the limited computational power (plus exponentially increasing time), I chose to leave depth 4 for the game demostration purposes. Ideally, I would comprate the percentage of times when the game reached the winning state with a range of different depths. Such an experiment, however, would be very computationally expensive.\n",
    "        \n",
    "\n",
    "* **Reinforcement learning**. One curious improvement that I read about is reinforcement learning. This application does not require the use of heuristics. Instead, the algorithm 'learns' the utility of certain steps overtime, i.e. as the time passed since the start increases. The problem with that implementation is that it requires a lot of memory and calls for extra efficient sorting methods (Lui, Prost & Lee, 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CS188 Spring 2014. (2014). Lecture 7: Expectimax [Video file]. Retrieved from https://www.youtube.com/watch?v=jaFRyzp7yWw\n",
    "\n",
    "\n",
    "2. Lui, M. H., Prost, A., Lee, C. W. (2017). COMP3211 Final Project Report: Solving the Game ‘2048’ Using Various Machine-Learning Agents. Hong Kong University of Science and Technology. Retrieved from https://home.cse.ust.hk/~yqsong/teaching/comp3211/projects/2017Fall/G13.pdf\n",
    "\n",
    "\n",
    "3. Pozzotti, N. (2014). An Artificial Intelligence for the 2048 game. Retrieved from https://nicola17.github.io/2014/03/26/an-artificial-intelligence-for-2048-game.html\n",
    "\n",
    "\n",
    "4. Shoaran, M. (n.d.). Expectimax Search. University of Victoria. Retrieved from \n",
    "https://web.uvic.ca/~maryam/AISpring94/Slides/06_ExpectimaxSearch.pdf\n",
    "\n",
    "\n",
    "5. Xiao, R. (2014). What is the optimal algorithm for the game 2048?. Stack Overflow. Retrieved from https://stackoverflow.com/a/22498940"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
